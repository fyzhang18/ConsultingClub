---
title: "Mindprint - Math"
author: "Felicia Zhang"
date: '2018-07-25'
output: pdf_document
toc: yes
toc_depth: 2
fontsize: 12pt
fig_height: 5
fig_width: 7
---

```{r setup, include=FALSE, warning=FALSE}
library(ggplot2) 
library(zoo)
library(reshape)
library(plyr)
library(dplyr)
library(scales) 
library(data.table)
library(signal)
library(matrixStats)
library(lme4)
library(arm)
library(broom)
library(tidyr)
library(wesanderson)
library(readxl)    
library(extrafont)

## 1. Load ACT/SAT data

#LOAD MULTIPLE EXCEL SHEETS INTO SEPARATE DFS

#get names of sheets
sheets <- readxl::excel_sheets("~/Documents/Felicia Zhang/Felicia/Princeton/ConsultingClub/Mindprint/Math.xlsx")

#load in excel data as 1 big file
lst <- lapply(sheets, function(sheet) 
  readxl::read_excel("~/Documents/Felicia Zhang/Felicia/Princeton/ConsultingClub/Mindprint/Math.xlsx", sheet = sheet)
)

#prepare names
names(lst) <- sheets

#turn it into DF
list2env(lst, envir = .GlobalEnv)

#difference between 70C, 72C, Combo v2.4: three different tests based on where we had sufficient student data, noting the one called the Combo is a mixture of SAT and ACT questions.

#0 = incorrect, 1 = correct, 2 = skip

## 2. Load mindprint data
mindprintdata <- read.csv("~/Documents/Felicia Zhang/Felicia/Princeton/ConsultingClub/Mindprint/mindprint_data.csv", header=T)
```

```{r, echo=FALSE, warning=FALSE, include=FALSE}
## SET UP DF
names(`70C`)[2:61] = 1:60
names(`72C`)[2:61] = 1:60

# Mindprint subjects
subs <- unique(mindprintdata$student_id)

# duplicates = subID 441, 8826, 4573, 

# Find matching subjects in 70C
foo <- `70C`[22:47,]
foo[] <- lapply(foo, function(x) as.numeric(as.character(x)))
colnames(foo)[1] <- "student_id"

foofinal1 <- merge(mindprintdata, foo)

# Find matching subjects in 72C
foo <- `72C`[22:67,]
foo[] <- lapply(foo, function(x) as.numeric(as.character(x)))
colnames(foo)[1] <- "student_id"

foofinal2 <- merge(mindprintdata, foo)

finalDF <- rbind(foofinal1,foofinal2)

# calculate ACT math score (total number of questions = 60)
finalDF$ACTmathraw <- rowSums(finalDF[43:102])
finalDF$ACTmathscore <- finalDF$ACTmathraw / 60

# remove missing data
finalDF2 <- na.omit(finalDF)
```

```{r, echo=FALSE, warning=FALSE, include=FALSE}
#set up subsection DF
foo <- `70C`[1:4,]
subsection <- t(foo)
subsection <- data.frame(subsection)
subsection <- subsection[-1, ] 

colnames(subsection)[1] <- "test"
colnames(subsection)[2] <- "question"
colnames(subsection)[3] <- "section"
colnames(subsection)[4] <- "subsection"

EA <- subset(subsection, subsection=="EA") #pre algebra
GT <- subset(subsection, subsection=="GT") #geometry/trig
AG <- subset(subsection, subsection=="AG") #intermediate algebra

EAnum <- unique(EA$question)
EAnum <- as.numeric(EAnum)
GTnum <- unique(GT$question)
GTnum <- as.numeric(GTnum)
AGnum <- unique(AG$question)
AGnum <- as.numeric(AGnum)

#all questions
questions <- finalDF2[,43:102]

#AG DF
AG_DF <- questions[,AGnum]
AG_DF$AGraw <- rowSums(AG_DF)
AG_DF$AGscore <- AG_DF$AGraw / length(AGnum)
AG_DF <- cbind(finalDF2[,1:42],AG_DF)

#EA DF
EA_DF <- questions[,EAnum]
EA_DF$EAraw <- rowSums(EA_DF)
EA_DF$EAscore <- EA_DF$EAraw / length(EAnum)
EA_DF <- cbind(finalDF2[,1:42],EA_DF)

#GT DF
GT_DF <- questions[,GTnum]
GT_DF$GTraw <- rowSums(GT_DF)
GT_DF$GTscore <- GT_DF$GTraw / length(GTnum)
GT_DF <- cbind(finalDF2[,1:42],GT_DF)
```
## From looking at correlations we found:

1. Overall Math Score significantly correlates with:
* spatial perception accuracy (r = 0.33, p = 0.008)
* WM efficiency (r = 0.26, p = 0.04)

2. Pre-Algebra/Elementary Algebra significantly correlates with:
* spatial perception accuracy (r = 0.41, p = 0.0009)

3. Plane Geometry/Trignometry significantly correlates with:
* spatial perception accuracy (r = 0.28, p=0.027)

4. Intermediate Algebra/Geometry significantly correlates with: 
* WM efficiency (r = 0.3, p = 0.017)
* WM accuracy (r = 0.3, p = 0.018)

\newpage

## Regression models
### Overall Math Score
```{r, echo=FALSE, warning=FALSE}

overallmath_model <- lm(ACTmathscore ~ SPA_Az + WM_EFFICIENCY, data=finalDF2)  

summary(overallmath_model)

```

Spatial perception accuracy is a stronger predictor of overall math score compared to working memory efficiency

\newpage

### Pre-Algebra/Elementary Algebra
```{r, echo=FALSE, warning=FALSE}

prealgebra_model <- lm(EAscore ~ SPA_Az, data=EA_DF)  
summary(prealgebra_model)

```

Spatial perception accuracy is a very strong predictor of Pre-Algebra/Elementary Algebra score.

\newpage

### Plane Geometry/Trignometry
```{r, echo=FALSE, warning=FALSE}

geo_model <- lm(GTscore ~ SPA_Az, data=GT_DF)  
summary(geo_model)

```

Spatial perception accuracy is a strong predictor of plane geometry/trig.

\newpage

### Intermediate Algebra/Geometry
```{r, echo=FALSE, warning=FALSE}

interalgebra_model <- lm(AGscore ~ WM_EFFICIENCY, data=AG_DF)  
summary(interalgebra_model)

interalgebra_model2 <- lm(AGscore ~ WM_Az, data=AG_DF)  
summary(interalgebra_model2)
```

Working memory accuracy and efficiency are not longer significant when you include both in the same model, but that's because efficiency is accuracy + RT averaged. So we can just look at efficiency.

\newpage

